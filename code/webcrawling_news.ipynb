{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c00f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "입력 형식에 맞게 입력해주세요.\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "==================================================\n",
      "최대 크롤링할 페이지 수 입력하시오: 10\n",
      "검색어 입력: 클린스만\n",
      "뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): 0\n",
      "시작날짜 입력(2024.01.01):2024.01.15\n",
      "끝날짜 입력(2024.02.01):2024.01.30\n",
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#각 크롤링 결과 저장하기 위한 리스트 선언 \n",
    "title_text = []\n",
    "link_text = []\n",
    "source_text = []\n",
    "date_text = []\n",
    "contents_text = []\n",
    "result = {}\n",
    "\n",
    "#엑셀로 저장하기 위한 변수\n",
    "RESULT_PATH ='C:/Users/임채현'  #결과 저장할 경로\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기\n",
    "\n",
    "#날짜 정제화 함수\n",
    "def date_cleansing(test):    \n",
    "    try:        \n",
    "        #지난 뉴스                 \n",
    "        pattern = '\\d+.(\\d+).(\\d+).'  #정규표현식             \n",
    "        \n",
    "        r = re.compile(pattern)        \n",
    "        match = r.search(test).group(0)  # 2024.01.01       \n",
    "        date_text.append(match)            \n",
    "        \n",
    "    except AttributeError:        \n",
    "        #최근 뉴스                \n",
    "        pattern = '\\w* (\\d\\w*)'     #정규표현식                 \n",
    "        \n",
    "        r = re.compile(pattern)        \n",
    "        match = r.search(test).group(1)        \n",
    "        #print(match)        \n",
    "        date_text.append(match)\n",
    "\n",
    "#내용 정제화 함수 \n",
    "def contents_cleansing(contents):    \n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '',                                       \n",
    "                                      str(contents)).strip()  #앞에 필요없는 부분 제거    \n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '',                                        \n",
    "                                       first_cleansing_contents).strip() #뒤에 필요없는 부분 제거 (새끼 기사)    \n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()    \n",
    "    contents_text.append(third_cleansing_contents)    \n",
    "    #print(contents_text)\n",
    "\n",
    "def crawler(maxpage,query,sort,s_date,e_date):     \n",
    "    s_from = s_date.replace(\".\",\"\")    \n",
    "    e_to = e_date.replace(\".\",\"\")    \n",
    "    page = 1      \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지        \n",
    "    \n",
    "    while page <= maxpage_t:        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)                \n",
    "        \n",
    "        response = requests.get(url)        \n",
    "        html = response.text         \n",
    "        \n",
    "        #뷰티풀소프의 인자값 지정        \n",
    "        soup = BeautifulSoup(html, 'html.parser')         \n",
    "        \n",
    "        #<a>태그에서 제목과 링크주소 추출        \n",
    "        atags = soup.select('.news_tit')        \n",
    "        for atag in atags:            \n",
    "            title_text.append(atag.text)     #제목            \n",
    "            link_text.append(atag['href'])   #링크주소                    \n",
    "            \n",
    "        #신문사 추출        \n",
    "        source_lists = soup.select('.info_group > .press')        \n",
    "        for source_list in source_lists:            \n",
    "            source_text.append(source_list.text)    #신문사   \n",
    "            \n",
    "        #날짜 추출         \n",
    "        date_lists = soup.select('.info_group > span.info')        \n",
    "        \n",
    "        for date_list in date_lists:            \n",
    "            # 1면 3단 같은 위치 제거            \n",
    "            if date_list.text.find(\"면\") == -1:                \n",
    "                date_text.append(date_list.text)                \n",
    "            \n",
    "        #본문요약본        \n",
    "        contents_lists = soup.select('.news_dsc')        \n",
    "        for contents_list in contents_lists:            \n",
    "            contents_cleansing(contents_list) #본문요약 정제화                 \n",
    "            \n",
    "        #모든 리스트 딕셔너리형태로 저장        \n",
    "        result= {\"date\" : date_text , \"title\":title_text ,  \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }          \n",
    "        print(page)                \n",
    "        \n",
    "        df = pd.DataFrame(result)  #df로 변환        \n",
    "        page += 10            # 새로 만들 파일이름 지정    \n",
    "        \n",
    "    # 새로 만들 파일이름 지정    \n",
    "    outputFileName = '/16강.csv'    \n",
    "    df.to_csv(RESULT_PATH+outputFileName,index=False)\n",
    "\n",
    "def main():    \n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)        \n",
    "    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")      \n",
    "    query = input(\"검색어 입력: \")      \n",
    "    sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2    \n",
    "    s_date = input(\"시작날짜 입력(2024.01.01):\")  #2024.01.01   \n",
    "    e_date = input(\"끝날짜 입력(2024.02.01):\")   #2024.02.01       \n",
    "    crawler(maxpage,query,sort,s_date,e_date)    \n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
